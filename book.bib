@misc{keybert,
  author       = {Maarten Grootendorst},
  title        = {KeyBERT: Minimal keyword extraction with BERT.},
  year         = 2020,
  publisher    = {Zenodo},
  version      = {v0.3.0},
  doi          = {10.5281/zenodo.4461265},
  url          = {https://doi.org/10.5281/zenodo.4461265}
}

@inproceedings{textrank,
    title = "{T}ext{R}ank: Bringing Order into Text",
    author = "Mihalcea, Rada  and
      Tarau, Paul",
    booktitle = "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-3252",
    pages = "404--411",
}

@inproceedings{augenstein-etal-2017-semeval,
    title = "{S}em{E}val 2017 Task 10: {S}cience{IE} - Extracting Keyphrases and Relations from Scientific Publications",
    author = "Augenstein, Isabelle  and
      Das, Mrinal  and
      Riedel, Sebastian  and
      Vikraman, Lakshmi  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S17-2091",
    doi = "10.18653/v1/S17-2091",
    pages = "546--555",
    abstract = "We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities.",
}
@inproceedings{inspec,
author = {Hulth, Anette},
title = {Improved Automatic Keyword Extraction given More Linguistic Knowledge},
year = {2003},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1119355.1119383},
doi = {10.3115/1119355.1119383},
abstract = {In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed. The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on statistics (such as term frequency and n-grams), a better result is obtained as measured by keywords previously assigned by professional indexers. In more detail, extracting NP-chunks gives a better precision than n-grams, and by adding the PoS tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.},
booktitle = {Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing},
pages = {216–223},
numpages = {8},
series = {EMNLP '03}
}

@article{summa,
  author    = {Federico Barrios and
             Federico L{\'{o}}pez and
             Luis Argerich and
             Rosa Wachenchauzer},
  title     = {Variations of the Similarity Function of TextRank for Automated Summarization},
  journal   = {CoRR},
  volume    = {abs/1602.03606},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.03606},
  archivePrefix = {arXiv},
  eprint    = {1602.03606},
  timestamp = {Wed, 07 Jun 2017 14:40:43 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BarriosLAW16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{yake1,
title = {YAKE! Keyword extraction from single documents using multiple local features},
journal = {Information Sciences},
volume = {509},
pages = {257-289},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519308588},
author = {Ricardo Campos and Vítor Mangaravite and Arian Pasquali and Alípio Jorge and Célia Nunes and Adam Jatowt},
keywords = {Keyword extraction, Information extraction, Unsupervised Algorithm},
abstract = {As the amount of generated information grows, reading and summarizing texts of large collections turns into a challenging task. Many documents do not come with descriptive terms, thus requiring humans to generate keywords on-the-fly. The need to automate this kind of task demands the development of keyword extraction systems with the ability to automatically identify keywords within the text. One approach is to resort to machine-learning algorithms. These, however, depend on large annotated text corpora, which are not always available. An alternative solution is to consider an unsupervised approach. In this article, we describe YAKE!, a light-weight unsupervised automatic keyword extraction method which rests on statistical text features extracted from single documents to select the most relevant keywords of a text. Our system does not need to be trained on a particular set of documents, nor does it depend on dictionaries, external corpora, text size, language, or domain. To demonstrate the merits and significance of YAKE!, we compare it against ten state-of-the-art unsupervised approaches and one supervised method. Experimental results carried out on top of twenty datasets show that YAKE! significantly outperforms other unsupervised methods on texts of different sizes, languages, and domains.}
}

@article{yake2,
author = {Campos, Ricardo and Mangaravite, V\'{\i}tor and Pasquali, Arian and Jorge, Al\'{\i}pio and Nunes, C\'{e}lia and Jatowt, Adam},
title = {YAKE! Keyword Extraction from Single Documents Using Multiple Local Features},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {509},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.09.013},
doi = {10.1016/j.ins.2019.09.013},
journal = {Inf. Sci.},
month = {jan},
pages = {257–289},
numpages = {33},
keywords = {Keyword extraction, Unsupervised Algorithm, Information extraction}
}

@InProceedings{yake3,
author="Campos, Ricardo
and Mangaravite, V{\'i}tor
and Pasquali, Arian
and Jorge, Al{\'i}pio M{\'a}rio
and Nunes, C{\'e}lia
and Jatowt, Adam",
editor="Pasi, Gabriella
and Piwowarski, Benjamin
and Azzopardi, Leif
and Hanbury, Allan",
title="YAKE! Collection-Independent Automatic Keyword Extractor",
booktitle="Advances in Information Retrieval",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="806--810",
abstract="In this paper, we present YAKE!, a novel feature-based system for multi-lingual keyword extraction from single documents, which supports texts of different sizes, domains or languages. Unlike most systems, YAKE! does not rely on dictionaries or thesauri, neither it is trained against any corpora. Instead, we follow an unsupervised approach which builds upon features extracted from the text, making it thus applicable to documents written in many different languages without the need for external knowledge. This can be beneficial for a large number of tasks and a plethora of situations where the access to training corpora is either limited or restricted. In this demo, we offer an easy to use, interactive session, where users from both academia and industry can try our system, either by using a sample document or by introducing their own text. As an add-on, we compare our extracted keywords against the output produced by the IBM Natural Language Understanding (IBM NLU) and Rake system. YAKE! demo is available at http://bit.ly/YakeDemoECIR2018. A python implementation of YAKE! is also available at PyPi repository (https://pypi.python.org/pypi/yake/).",
isbn="978-3-319-76941-7"
}

@article{DBLP,
  author    = {Bijoyan Das and
               Sarit Chakraborty},
  title     = {An Improved Text Sentiment Classification Model Using {TF-IDF} and
               Next Word Negation},
  journal   = {CoRR},
  volume    = {abs/1806.06407},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.06407},
  eprinttype = {arXiv},
  eprint    = {1806.06407},
  timestamp = {Mon, 13 Aug 2018 16:47:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-06407.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{234,
author = {Qaiser, Shahzad and Ali, Ramsha},
year = {2018},
month = {07},
pages = {},
title = {Text Mining: Use of TF-IDF to Examine the Relevance of Words to Documents},
volume = {181},
journal = {International Journal of Computer Applications},
doi = {10.5120/ijca2018917395}
}

@article{GRAVES2005602,
title = {Framewise phoneme classification with bidirectional LSTM and other neural network architectures},
journal = {Neural Networks},
volume = {18},
number = {5},
pages = {602-610},
year = {2005},
note = {IJCNN 2005},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2005.06.042},
url = {https://www.sciencedirect.com/science/article/pii/S0893608005001206},
author = {Alex Graves and Jürgen Schmidhuber},
abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it.11An abbreviated version of some portions of this article appeared in (Graves and Schmidhuber, 2005), as part of the IJCNN 2005 conference proceedings, published under the IEEE copyright.}
}